import time
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
import threading
import queue
from dataclasses import dataclass
import torch


@dataclass
class KVCache:
    """Representation of a KV cache entry"""

    key_states: torch.Tensor
    value_states: torch.Tensor
    request_id: str
    last_accessed: float
    size_bytes: int

    def update_access_time(self):
        self.last_accessed = time.time()


class MemoryPool:
    """Elastic memory pool for managing GPU memory across multiple devices"""

    def __init__(self, devices: List[int], memory_per_device: List[int]):
        """
        Initialize memory pool

        Args:
            devices: List of GPU device IDs
            memory_per_device: Memory allocation per device in MB
        """
        self.devices = devices
        self.memory_per_device = memory_per_device
        self.total_memory = sum(memory_per_device)
        self.available_memory = self.total_memory
        self.device_usage = {device: 0 for device in devices}
        self.lock = threading.Lock()

    def allocate(
        self, size_mb: int, preferred_device: Optional[int] = None
    ) -> Tuple[bool, Optional[int]]:
        """
        Allocate memory from the pool

        Args:
            size_mb: Size to allocate in MB
            preferred_device: Preferred GPU device, if any

        Returns:
            Tuple of (success, device_id)
        """
        with self.lock:
            if size_mb > self.available_memory:
                return False, None

            if (
                preferred_device is not None
                and self.device_usage[preferred_device] + size_mb
                <= self.memory_per_device[self.devices.index(preferred_device)]
            ):
                self.device_usage[preferred_device] += size_mb
                self.available_memory -= size_mb
                return True, preferred_device

            # Find device with most free memory
            best_device = None
            most_free = -1

            for i, device in enumerate(self.devices):
                free_memory = self.memory_per_device[i] - self.device_usage[device]
                if free_memory >= size_mb and free_memory > most_free:
                    most_free = free_memory
                    best_device = device

            if best_device is not None:
                self.device_usage[best_device] += size_mb
                self.available_memory -= size_mb
                return True, best_device

            return False, None

    def free(self, size_mb: int, device: int):
        """Free memory from a specific device"""
        with self.lock:
            self.device_usage[device] = max(0, self.device_usage[device] - size_mb)
            self.available_memory += size_mb


class CacheManager:
    """Manages KV cache across devices with eviction policies"""

    def __init__(
        self,
        memory_pool: MemoryPool,
        cache_capacity_percent: float = 80.0,
        eviction_policy: str = "LRU",
    ):
        """
        Initialize cache manager

        Args:
            memory_pool: The shared memory pool
            cache_capacity_percent: Percentage of memory pool to use for caching
            eviction_policy: Cache eviction policy (LRU, FIFO, etc.)
        """
        self.memory_pool = memory_pool
        self.cache_capacity = int(
            memory_pool.total_memory * (cache_capacity_percent / 100.0)
        )
        self.current_cache_size = 0
        self.eviction_policy = eviction_policy
        self.cache_entries: Dict[str, Dict[str, KVCache]] = (
            {}
        )  # request_id -> {layer_id -> KVCache}
        self.device_mapping: Dict[str, int] = {}  # request_id -> device_id
        self.lock = threading.Lock()

    def store(
        self,
        request_id: str,
        layer_id: str,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        preferred_device: Optional[int] = None,
    ) -> bool:
        """
        Store KV cache for a request and layer

        Args:
            request_id: Unique request identifier
            layer_id: Layer identifier
            key_states: Key states tensor
            value_states: Value states tensor
            preferred_device: Preferred device for storage (if any)

        Returns:
            Success flag
        """
        # Calculate memory size in MB
        key_size = key_states.element_size() * key_states.nelement() / (1024 * 1024)
        value_size = (
            value_states.element_size() * value_states.nelement() / (1024 * 1024)
        )
        total_size = key_size + value_size

        with self.lock:
            # Check if we already have an entry for this request
            if request_id in self.cache_entries:
                device = self.device_mapping[request_id]

                # If this layer is already cached, update it
                if layer_id in self.cache_entries[request_id]:
                    old_cache = self.cache_entries[request_id][layer_id]
                    old_size = old_cache.size_bytes / (1024 * 1024)

                    # Free the old allocation
                    self.memory_pool.free(old_size, device)
                    self.current_cache_size -= old_size

                    # Try to allocate new size
                    success, device = self.memory_pool.allocate(total_size, device)
                    if not success:
                        # Try to evict if allocation failed
                        self._evict(total_size - old_size)
                        success, device = self.memory_pool.allocate(total_size, device)
                        if not success:
                            return False

                    # Update cache entry
                    cache_entry = KVCache(
                        key_states=key_states.to(f"cuda:{device}"),
                        value_states=value_states.to(f"cuda:{device}"),
                        request_id=request_id,
                        last_accessed=time.time(),
                        size_bytes=int(total_size * 1024 * 1024),
                    )
                    self.cache_entries[request_id][layer_id] = cache_entry
                    self.current_cache_size += total_size
                    return True
            else:
                # New request - try to allocate
                if preferred_device is None and len(self.cache_entries) > 0:
                    # Try to use the same device as other layers from this request
                    similar_requests = [
                        req
                        for req in self.device_mapping.keys()
                        if req.split("_")[0] == request_id.split("_")[0]
                    ]
                    if similar_requests:
                        preferred_device = self.device_mapping[similar_requests[0]]

                success, device = self.memory_pool.allocate(
                    total_size, preferred_device
                )
                if not success:
                    # Try to evict
                    self._evict(total_size)
                    success, device = self.memory_pool.allocate(
                        total_size, preferred_device
                    )
                    if not success:
                        return False

                # Create new cache entry
                cache_entry = KVCache(
                    key_states=key_states.to(f"cuda:{device}"),
                    value_states=value_states.to(f"cuda:{device}"),
                    request_id=request_id,
                    last_accessed=time.time(),
                    size_bytes=int(total_size * 1024 * 1024),
                )

                if request_id not in self.cache_entries:
                    self.cache_entries[request_id] = {}
                    self.device_mapping[request_id] = device

                self.cache_entries[request_id][layer_id] = cache_entry
                self.current_cache_size += total_size
                return True

    def retrieve(
        self, request_id: str, layer_id: str
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        Retrieve KV cache for a request and layer

        Args:
            request_id: Unique request identifier
            layer_id: Layer identifier

        Returns:
            Tuple of (key_states, value_states) or (None, None) if not found
        """
        with self.lock:
            if (
                request_id in self.cache_entries
                and layer_id in self.cache_entries[request_id]
            ):
                cache_entry = self.cache_entries[request_id][layer_id]
                cache_entry.update_access_time()
                return cache_entry.key_states, cache_entry.value_states
            return None, None

    def _evict(self, required_mb: float) -> bool:
        """
        Evict cache entries to free up required memory

        Args:
            required_mb: Required memory in MB

        Returns:
            Success flag
        """
        if self.current_cache_size == 0:
            return False

        if self.eviction_policy == "LRU":
            # Get all cache entries flattened
            all_entries = []
            for request_id, layers in self.cache_entries.items():
                for layer_id, cache in layers.items():
                    all_entries.append((request_id, layer_id, cache))

            # Sort by last accessed time
            all_entries.sort(key=lambda x: x[2].last_accessed)

            freed_mb = 0
            evicted = []

            # Evict oldest entries first
            for request_id, layer_id, cache in all_entries:
                entry_size_mb = cache.size_bytes / (1024 * 1024)
                device = self.device_mapping[request_id]
                self.memory_pool.free(entry_size_mb, device)
                freed_mb += entry_size_mb
                evicted.append((request_id, layer_id))

                if freed_mb >= required_mb:
                    break

            # Remove evicted entries from cache
            for request_id, layer_id in evicted:
                del self.cache_entries[request_id][layer_id]
                if not self.cache_entries[request_id]:
                    del self.cache_entries[request_id]
                    del self.device_mapping[request_id]

            self.current_cache_size -= freed_mb
            return freed_mb >= required_mb

        elif self.eviction_policy == "FIFO":
            # Implement FIFO eviction if needed
            pass

        return False

    def clear(self, request_id: Optional[str] = None):
        """
        Clear cache entries

        Args:
            request_id: Specific request to clear, or all if None
        """
        with self.lock:
            if request_id is not None:
                if request_id in self.cache_entries:
                    device = self.device_mapping[request_id]
                    for layer_id, cache in self.cache_entries[request_id].items():
                        entry_size_mb = cache.size_bytes / (1024 * 1024)
                        self.memory_pool.free(entry_size_mb, device)
                        self.current_cache_size -= entry_size_mb

                    del self.cache_entries[request_id]
                    del self.device_mapping[request_id]
            else:
                # Clear all cache entries
                for request_id, layers in self.cache_entries.items():
                    device = self.device_mapping[request_id]
                    for layer_id, cache in layers.items():
                        entry_size_mb = cache.size_bytes / (1024 * 1024)
                        self.memory_pool.free(entry_size_mb, device)

                self.current_cache_size = 0
                self.cache_entries.clear()
                self.device_mapping.clear()


class RequestScheduler:
    """Schedules and routes inference requests"""

    def __init__(self, memory_pool: MemoryPool, cache_manager: CacheManager):
        """
        Initialize request scheduler

        Args:
            memory_pool: The shared memory pool
            cache_manager: The KV cache manager
        """
        self.memory_pool = memory_pool
        self.cache_manager = cache_manager
        self.request_queue = queue.PriorityQueue()
        self.active_requests = set()
        self.lock = threading.Lock()
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop)
        self.running = False

    def start(self):
        """Start the scheduler thread"""
        self.running = True
        self.scheduler_thread.start()

    def stop(self):
        """Stop the scheduler thread"""
        self.running = False
        if self.scheduler_thread.is_alive():
            self.scheduler_thread.join()

    def submit_request(
        self, request_id: str, priority: int, prefill_tokens: int, decode_tokens: int
    ):
        """
        Submit a request for scheduling

        Args:
            request_id: Unique request identifier
            priority: Request priority (lower value = higher priority)
            prefill_tokens: Number of tokens for prefill phase
            decode_tokens: Number of tokens for decode phase
        """
        with self.lock:
            self.request_queue.put(
                (priority, time.time(), request_id, prefill_tokens, decode_tokens)
            )

    def _scheduler_loop(self):
        """Main scheduler loop"""
        while self.running:
            try:
                if self.request_queue.empty():
                    time.sleep(0.01)
                    continue

                priority, submit_time, request_id, prefill_tokens, decode_tokens = (
                    self.request_queue.get(block=False)
                )

                # Check if we can process this request
                with self.lock:
                    # Estimate memory requirements
                    # In a real implementation, this would be based on model specs
                    prefill_memory_mb = prefill_tokens * 0.1  # Example estimate
                    decode_memory_mb = decode_tokens * 0.05  # Example estimate

                    # Check cache hit
                    cache_hit = False
                    for layer_id in [
                        f"layer_{i}" for i in range(12)
                    ]:  # Example with 12 layers
                        k, v = self.cache_manager.retrieve(request_id, layer_id)
                        if k is not None and v is not None:
                            cache_hit = True
                            break

                    # Adjust memory requirements based on cache hits
                    if cache_hit:
                        required_memory = decode_memory_mb
                    else:
                        required_memory = prefill_memory_mb + decode_memory_mb

                    # Try to allocate
                    success, device = self.memory_pool.allocate(required_memory)

                    if success:
                        # Process the request (in a real implementation, this would launch processing)
                        self.active_requests.add(request_id)
                        print(f"Processing request {request_id} on device {device}")

                        # Simulate processing
                        threading.Thread(
                            target=self._process_request,
                            args=(request_id, device, required_memory),
                        ).start()
                    else:
                        # Re-queue the request with the same priority
                        self.request_queue.put(
                            (
                                priority,
                                submit_time,
                                request_id,
                                prefill_tokens,
                                decode_tokens,
                            )
                        )
                        time.sleep(0.1)  # Avoid tight loop

            except queue.Empty:
                pass
            except Exception as e:
                print(f"Error in scheduler loop: {e}")
                time.sleep(0.1)

    def _process_request(self, request_id: str, device: int, memory_mb: float):
        """
        Simulate processing a request

        Args:
            request_id: Request identifier
            device: Assigned device
            memory_mb: Allocated memory in MB
        """
        try:
            # Simulate processing time
            time.sleep(np.random.uniform(0.5, 2.0))

            # In a real implementation, this would run model inference
            # and store KV cache in the cache manager

            # Example of storing cache for the request (just a simulation)
            for layer_id in [f"layer_{i}" for i in range(12)]:
                # Create mock tensors
                key_size = [2, 8, 16, 32]  # batch, heads, seq_len, head_dim
                value_size = [2, 8, 16, 32]

                key_states = torch.rand(key_size, device=f"cuda:{device}")
                value_states = torch.rand(value_size, device=f"cuda:{device}")

                self.cache_manager.store(
                    request_id=request_id,
                    layer_id=layer_id,
                    key_states=key_states,
                    value_states=value_states,
                    preferred_device=device,
                )

            # Free the allocated memory
            with self.lock:
                if request_id in self.active_requests:
                    self.active_requests.remove(request_id)
                self.memory_pool.free(memory_mb, device)

        except Exception as e:
            print(f"Error processing request {request_id}: {e}")
            # Ensure memory is freed even on error
            with self.lock:
                if request_id in self.active_requests:
                    self.active_requests.remove(request_id)
                self.memory_pool.free(memory_mb, device)


class ModelRunner:
    """Runs model inference with memory-aware execution"""

    def __init__(self, model_path: str, cache_manager: CacheManager):
        """
        Initialize model runner

        Args:
            model_path: Path to model weights
            cache_manager: KV cache manager
        """
        self.model_path = model_path
        self.cache_manager = cache_manager
        # In a real implementation, this would load the model

    def run_inference(
        self, request_id: str, input_tokens: List[int], max_new_tokens: int, device: int
    ) -> List[int]:
        """
        Run model inference

        Args:
            request_id: Request identifier
            input_tokens: Input token IDs
            max_new_tokens: Maximum number of tokens to generate
            device: Device to run on

        Returns:
            Generated token IDs
        """
        # This is a simplified example. In a real implementation,
        # this would run the actual model inference.

        # Simulate token generation
        generated_tokens = []

        # Check if we have cached KV states
        cache_hit = True
        for layer_id in [f"layer_{i}" for i in range(12)]:
            k, v = self.cache_manager.retrieve(request_id, layer_id)
            if k is None or v is None:
                cache_hit = False
                break

        # Simulate prefill if no cache hit
        if not cache_hit:
            print(f"Cache miss for request {request_id}. Running prefill.")
            # Simulate prefill computation
            time.sleep(len(input_tokens) * 0.01)

            # Create and store KV cache
            for layer_id in [f"layer_{i}" for i in range(12)]:
                seq_len = len(input_tokens)

                # Create mock tensors
                key_size = [1, 8, seq_len, 32]  # batch, heads, seq_len, head_dim
                value_size = [1, 8, seq_len, 32]

                key_states = torch.rand(key_size, device=f"cuda:{device}")
                value_states = torch.rand(value_size, device=f"cuda:{device}")

                self.cache_manager.store(
                    request_id=request_id,
                    layer_id=layer_id,
                    key_states=key_states,
                    value_states=value_states,
                    preferred_device=device,
                )
        else:
            print(f"Cache hit for request {request_id}.")

        # Simulate decoding
        for i in range(max_new_tokens):
            # In a real implementation, this would run the model's forward pass
            # using the cached KV states

            # Simulate decoding computation
            time.sleep(0.02)

            # Generate a random token (in a real implementation, this would be model output)
            next_token = np.random.randint(0, 50000)
            generated_tokens.append(next_token)

            # Update KV cache with the new token
            for layer_id in [f"layer_{i}" for i in range(12)]:
                # Get existing cache
                k, v = self.cache_manager.retrieve(request_id, layer_id)

                if k is not None and v is not None:
                    # Append new token's KV states
                    new_k = torch.cat(
                        [k, torch.rand(1, 8, 1, 32, device=k.device)], dim=2
                    )
                    new_v = torch.cat(
                        [v, torch.rand(1, 8, 1, 32, device=v.device)], dim=2
                    )

                    # Store updated cache
                    self.cache_manager.store(
                        request_id=request_id,
                        layer_id=layer_id,
                        key_states=new_k,
                        value_states=new_v,
                    )

            # Simulate end of text
            if next_token == 50000 - 1:  # EOS token in this simulation
                break

        return generated_tokens


class MemServer:
    """Main server implementation integrating all components"""

    def __init__(
        self,
        model_path: str,
        gpu_devices: List[int],
        memory_per_device: List[int],
        cache_capacity_percent: float = 80.0,
    ):
        """
        Initialize MemServer

        Args:
            model_path: Path to model weights
            gpu_devices: List of GPU device IDs to use
            memory_per_device: Memory allocation per device in MB
            cache_capacity_percent: Percentage of memory for caching
        """
        # Initialize memory pool
        self.memory_pool = MemoryPool(gpu_devices, memory_per_device)

        # Initialize cache manager
        self.cache_manager = CacheManager(
            memory_pool=self.memory_pool,
            cache_capacity_percent=cache_capacity_percent,
            eviction_policy="LRU",
        )

        # Initialize request scheduler
        self.scheduler = RequestScheduler(
            memory_pool=self.memory_pool, cache_manager=self.cache_manager
        )

        # Initialize model runner
        self.model_runner = ModelRunner(
            model_path=model_path, cache_manager=self.cache_manager
        )

        self.request_counter = 0
        self.lock = threading.Lock()

    def start(self):
        """Start the server"""
        self.scheduler.start()
        print(f"MemServer started with {len(self.memory_pool.devices)} devices")

    def stop(self):
        """Stop the server"""
        self.scheduler.stop()
        print("MemServer stopped")

    def process_request(
        self, input_text: str, priority: int = 0, max_new_tokens: int = 100
    ) -> str:
        """
        Process a new request

        Args:
            input_text: Input text
            priority: Request priority (lower value = higher priority)
            max_new_tokens: Maximum tokens to generate

        Returns:
            Generated text
        """
        with self.lock:
            self.request_counter += 1
            request_id = f"req_{self.request_counter}"

        # In a real implementation, this would tokenize the input
        input_tokens = list(range(len(input_text.split())))

        # Submit for scheduling
        self.scheduler.submit_request(
            request_id=request_id,
            priority=priority,
            prefill_tokens=len(input_tokens),
            decode_tokens=max_new_tokens,
        )

        # In a real implementation, this would wait for completion
        # and return the actual generated text

        # For this example, we'll just wait a bit and return a placeholder
        time.sleep(1.0)  # Simulate waiting for completion
        return (
            f"Response for '{input_text[:20]}...' (Generated {max_new_tokens} tokens)"
        )


# Example usage
def run_example():
    # Initialize server
    server = MemServer(
        model_path="meta-llama/Meta-Llama-2-13B",
        gpu_devices=[0, 1],  # Use 2 GPUs
        memory_per_device=[8000, 8000],  # 8GB per GPU
        cache_capacity_percent=80.0,
    )

    # Start the server
    server.start()

    try:
        # Process some requests
        requests = [
            "What is the capital of France?",
            "Explain quantum computing in simple terms.",
            "Write a short story about a robot.",
            "What are the main features of Python?",
            "How does photosynthesis work?",
            "What is the meaning of life?",
        ]

        for i, request_text in enumerate(requests):
            priority = i % 3  # Different priorities
            response = server.process_request(
                input_text=request_text, priority=priority, max_new_tokens=50 + i * 10
            )
            print(f"Request {i+1} (Priority {priority}): {response}")
            time.sleep(0.5)  # Space out requests

    finally:
        # Stop the server
        server.stop()


if __name__ == "__main__":
    run_example()
